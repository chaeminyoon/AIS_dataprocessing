"""
PHASE 2 (KD-Tree + ë‚ ì§œë³„ ì €ì¥ + SHP ì¶œë ¥): ê¶¤ì -ê²©ì ë§¤ì¹­ ì´ˆê³ ì† ì²˜ë¦¬
- ë‚ ì§œë³„ ì¦‰ì‹œ ì €ì¥ (ì¤‘ë‹¨ í›„ ì¬ê°œ ê°€ëŠ¥)
- ì „ì²´ ê²©ì í¬í•¨ (ë°ì´í„° ì—†ìœ¼ë©´ 0)
- â­ ìµœì í™”: GeoDataFrame ì¸ë±ì‹± ì˜¤ë²„í—¤ë“œ ì œê±°
- â­ SHP ì¶œë ¥: ê²©ìë¡œ ì˜ë¦° ê¶¤ì ì„ GIS í¬ë§·ìœ¼ë¡œ ì €ì¥
"""

import pandas as pd
import geopandas as gpd
from shapely.geometry import Point, LineString
from shapely import wkt
import json
import os
from datetime import datetime
from multiprocessing import Pool, cpu_count
import glob
import numpy as np
from scipy.spatial import cKDTree

# --- File Paths ---
BASE_PATH = r"/media/data1/cmyoon/AIS_process"
INPUT_DIR = os.path.join(BASE_PATH, "filtered_daily")
GRID_PATH = os.path.join(BASE_PATH, "grid_polygon_wkt.csv")
OUTPUT_DIR = os.path.join(BASE_PATH, "results")
DAILY_RESULTS_DIR = os.path.join(OUTPUT_DIR, "daily_grids")
DAILY_SHP_DIR = os.path.join(OUTPUT_DIR, "daily_trajectories_shp")  # â­ ìƒˆë¡œ ì¶”ê°€
FINAL_CSV = os.path.join(OUTPUT_DIR, "grid_trajectory_summary.csv")
FINAL_JSON = os.path.join(OUTPUT_DIR, "grid_trajectory_summary.json")
FINAL_SHP_DIR = os.path.join(OUTPUT_DIR, "all_trajectories_shp")  # â­ ìƒˆë¡œ ì¶”ê°€

os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(DAILY_RESULTS_DIR, exist_ok=True)
os.makedirs(DAILY_SHP_DIR, exist_ok=True)  # â­ ìƒˆë¡œ ì¶”ê°€
os.makedirs(FINAL_SHP_DIR, exist_ok=True)  # â­ ìƒˆë¡œ ì¶”ê°€

# ë©”ëª¨ë¦¬ íš¨ìœ¨: 10ê°œ í”„ë¡œì„¸ìŠ¤ (503GB ë©”ëª¨ë¦¬ ê³ ë ¤)
NUM_PROCESSES = 10  # ì•ˆì „í•œ ê°œìˆ˜

# â­ SHP ì¶œë ¥ ì˜µì…˜
SAVE_DAILY_SHP = True  # ë‚ ì§œë³„ SHP ì €ì¥
SAVE_FINAL_SHP = True  # ì „ì²´ í†µí•© SHP ì €ì¥

def create_trajectories(df):
    """Create trajectories for each ship_id."""
    df_sorted = df.sort_values(['decoded_ship_id', 'recv_dt']).copy()
    trajectories = []
    
    for ship_id, group in df_sorted.groupby('decoded_ship_id'):
        if len(group) < 2:
            continue
        try:
            points = [Point(row['lon_val'], row['lat_val']) for _, row in group.iterrows()]
            line = LineString(points)
            trajectories.append({
                'ship_id': ship_id,
                'geometry': line,
                'point_count': len(points)
            })
        except:
            continue
    
    return gpd.GeoDataFrame(trajectories, crs='EPSG:4326')

def build_grid_kdtree(gdf_grid):
    """ê²©ìì˜ ì¤‘ì‹¬ì ìœ¼ë¡œ KD-Tree ìƒì„±"""
    centroids = np.array([[geom.centroid.x, geom.centroid.y] 
                          for geom in gdf_grid.geometry])
    kdtree = cKDTree(centroids)
    return kdtree, centroids

def sample_trajectory_points(trajectory, max_distance=0.05):
    """ê¶¤ì ì„ ê· ë“±í•˜ê²Œ ìƒ˜í”Œë§"""
    if trajectory.length == 0:
        return []
    
    num_samples = max(10, int(trajectory.length / max_distance))
    num_samples = min(num_samples, 1000)
    
    distances = np.linspace(0, trajectory.length, num_samples)
    points = [trajectory.interpolate(d) for d in distances]
    
    return np.array([[p.x, p.y] for p in points])

def intersect_with_grid_kdtree_optimized(gdf_trajectories, gdf_grid, kdtree, grid_lookup, grid_bounds):
    """
    â­ ìµœì í™” ë²„ì „: KD-Treeë¥¼ ì´ìš©í•œ ì´ˆê³ ì† ê¶¤ì -ê²©ì ë§¤ì¹­
    
    ì£¼ìš” ê°œì„ :
    1. grid_lookup ë”•ì…”ë„ˆë¦¬ë¡œ O(1) ì ‘ê·¼
    2. grid_bounds ë¯¸ë¦¬ ê³„ì‚°
    3. ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë©”ëª¨ë¦¬ ê´€ë¦¬
    """
    if gdf_trajectories.empty or gdf_grid.empty:
        return gpd.GeoDataFrame()
    
    print(f"    Processing trajectories with KD-Tree (OPTIMIZED)...")
    start_time = datetime.now()
    
    results = []
    total_trajectories = len(gdf_trajectories)
    
    for idx, traj_row in gdf_trajectories.iterrows():
        # ì§„í–‰ ìƒí™© ì¶œë ¥ (100ê°œë§ˆë‹¤)
        if (idx + 1) % 100 == 0:
            elapsed = (datetime.now() - start_time).total_seconds()
            rate = (idx + 1) / elapsed if elapsed > 0 else 0
            remaining = (total_trajectories - idx - 1) / rate if rate > 0 else 0
            
            print(f"    [{idx + 1}/{total_trajectories}] ({(idx+1)/total_trajectories*100:.1f}%) "
                  f"| {rate:.1f} traj/sec | ETA: {remaining/60:.1f} min | "
                  f"Segments: {len(results):,}")
        
        ship_id = traj_row['ship_id']
        trajectory = traj_row['geometry']
        
        # ê¶¤ì  ìƒ˜í”Œë§
        sampled_points = sample_trajectory_points(trajectory)
        if len(sampled_points) == 0:
            continue
        
        # KD-Treeë¡œ í›„ë³´ ê²©ì ì°¾ê¸°
        search_radius = 0.1
        candidate_grid_indices = set()
        for point in sampled_points:
            nearby_indices = kdtree.query_ball_point(point, search_radius)
            candidate_grid_indices.update(nearby_indices)
        
        if not candidate_grid_indices:
            continue
        
        # ë°”ìš´ë”© ë°•ìŠ¤ í•„í„°
        traj_bounds = trajectory.bounds
        filtered_candidates = []
        for grid_idx in candidate_grid_indices:
            gb = grid_bounds[grid_idx]
            if not (gb[2] < traj_bounds[0] or gb[0] > traj_bounds[2] or
                    gb[3] < traj_bounds[1] or gb[1] > traj_bounds[3]):
                filtered_candidates.append(grid_idx)
        
        if not filtered_candidates:
            continue
        
        # â­ ê°œì„ : grid_lookupìœ¼ë¡œ O(1) ì ‘ê·¼
        for grid_idx in filtered_candidates:
            grid_info = grid_lookup[grid_idx]
            grid_geom = grid_info['geometry']
            grid_id = grid_info['grid_id']
            
            try:
                if not trajectory.intersects(grid_geom):
                    continue
                
                intersection = trajectory.intersection(grid_geom)
                if intersection.is_empty:
                    continue
                
                if intersection.geom_type == 'LineString':
                    results.append({
                        'grid_id': grid_id,
                        'ship_id': ship_id,
                        'geometry': intersection
                    })
                elif intersection.geom_type == 'MultiLineString':
                    for line in intersection.geoms:
                        if line.length > 0:
                            results.append({
                                'grid_id': grid_id,
                                'ship_id': ship_id,
                                'geometry': line
                            })
                elif intersection.geom_type == 'GeometryCollection':
                    for geom in intersection.geoms:
                        if geom.geom_type == 'LineString' and geom.length > 0:
                            results.append({
                                'grid_id': grid_id,
                                'ship_id': ship_id,
                                'geometry': geom
                            })
            except:
                continue
    
    if not results:
        return gpd.GeoDataFrame()
    
    return gpd.GeoDataFrame(results, crs='EPSG:4326')

def process_single_day(args):
    """Process a single day and save with ALL grids."""
    day_file, gdf_grid, kdtree, all_grid_ids, grid_lookup, grid_bounds = args
    day_num = os.path.basename(day_file).replace('filtered_', '').replace('.csv', '')
    
    # ì´ë¯¸ ì²˜ë¦¬ëœ ë‚ ì§œëŠ” ìŠ¤í‚µ
    output_file = os.path.join(DAILY_RESULTS_DIR, f"grid_summary_{day_num}.csv")
    if os.path.exists(output_file):
        print(f"[Day {day_num}] âœ“ Already processed, skipping")
        return output_file
    
    print(f"[Day {day_num}] Starting...")
    
    try:
        df = pd.read_csv(day_file, low_memory=False)
        if df.empty:
            print(f"[Day {day_num}] No data")
            empty_summary = pd.DataFrame({
                'ê²©ìëª…': all_grid_ids,
                'ì„ ë°•ì„ ê¸¸ì´sum': 0.0,
                'ì„ ì˜ê°¯ìˆ˜': 0
            })
            empty_summary.to_csv(output_file, index=False, encoding='utf-8-sig')
            print(f"[Day {day_num}] âœ“ Saved empty grid (all zeros)")
            return output_file
        
        print(f"[Day {day_num}] Loaded {len(df):,} rows")
        
        gdf_traj = create_trajectories(df)
        if gdf_traj.empty:
            print(f"[Day {day_num}] No trajectories")
            empty_summary = pd.DataFrame({
                'ê²©ìëª…': all_grid_ids,
                'ì„ ë°•ì„ ê¸¸ì´sum': 0.0,
                'ì„ ì˜ê°¯ìˆ˜': 0
            })
            empty_summary.to_csv(output_file, index=False, encoding='utf-8-sig')
            print(f"[Day {day_num}] âœ“ Saved empty grid")
            return output_file
        
        print(f"[Day {day_num}] Created {len(gdf_traj):,} trajectories")
        
        # â­ ìµœì í™”ëœ KD-Tree ê¸°ë°˜ êµì°¨ ê³„ì‚°
        gdf_int = intersect_with_grid_kdtree_optimized(gdf_traj, gdf_grid, kdtree, grid_lookup, grid_bounds)
        
        if gdf_int.empty:
            print(f"[Day {day_num}] No intersections")
            empty_summary = pd.DataFrame({
                'ê²©ìëª…': all_grid_ids,
                'ì„ ë°•ì„ ê¸¸ì´sum': 0.0,
                'ì„ ì˜ê°¯ìˆ˜': 0
            })
            empty_summary.to_csv(output_file, index=False, encoding='utf-8-sig')
            print(f"[Day {day_num}] âœ“ Saved empty grid")
            return output_file
        
        print(f"[Day {day_num}] Found {len(gdf_int):,} segments")
        
        # â­ SHP íŒŒì¼ ì €ì¥ (ë‚ ì§œë³„)
        if SAVE_DAILY_SHP:
            shp_output = os.path.join(DAILY_SHP_DIR, f"trajectories_{day_num}.shp")
            try:
                # EPSG:3857ë¡œ íˆ¬ì˜í•˜ì—¬ ì €ì¥ (ë¯¸í„° ë‹¨ìœ„ë¡œ ê±°ë¦¬ ê³„ì‚°í•˜ê¸° ì¢‹ìŒ)
                gdf_int_proj = gdf_int.to_crs('EPSG:3857')
                gdf_int_proj['length_m'] = gdf_int_proj.geometry.length
                
                # ì†ì„± ì¶”ê°€
                gdf_int_proj['length_m'] = gdf_int_proj['length_m'].round(2)
                
                # SHP ì €ì¥ (í•œê¸€ ì»¬ëŸ¼ëª…ì€ ìë™ìœ¼ë¡œ ASCIIë¡œ ë³€í™˜ë¨)
                gdf_int_proj.to_file(shp_output, driver='SHAPEFILE', encoding='utf-8')
                print(f"[Day {day_num}] âœ“ Saved SHP: {shp_output} ({len(gdf_int_proj):,} features)")
            except Exception as e:
                print(f"[Day {day_num}] âš ï¸ SHP save failed: {e}")
        
        # ê¸¸ì´ ê³„ì‚° (CSVìš©)
        gdf_int_proj = gdf_int.to_crs('EPSG:3857')
        gdf_int_proj['length_m'] = gdf_int_proj.geometry.length
        
        # ê²©ìë³„ ì§‘ê³„
        summary = gdf_int_proj.groupby('grid_id').agg(
            length_sum=('length_m', 'sum'),
            count=('length_m', 'count')
        ).reset_index()
        
        # â­ ì „ì²´ ê²©ìì™€ ë³‘í•© (ë°ì´í„° ì—†ëŠ” ê²©ìëŠ” 0ìœ¼ë¡œ)
        all_grids_df = pd.DataFrame({'grid_id': all_grid_ids})
        full_summary = all_grids_df.merge(summary, on='grid_id', how='left')
        full_summary['length_sum'] = full_summary['length_sum'].fillna(0)
        full_summary['count'] = full_summary['count'].fillna(0).astype(int)
        
        # ì»¬ëŸ¼ëª… ë³€ê²½
        full_summary.rename(columns={
            'grid_id': 'ê²©ìëª…',
            'length_sum': 'ì„ ë°•ì„ ê¸¸ì´sum',
            'count': 'ì„ ì˜ê°¯ìˆ˜'
        }, inplace=True)
        
        full_summary['ì„ ë°•ì„ ê¸¸ì´sum'] = full_summary['ì„ ë°•ì„ ê¸¸ì´sum'].round(2)
        
        # ì €ì¥
        full_summary.to_csv(output_file, index=False, encoding='utf-8-sig')
        
        data_grids = (full_summary['ì„ ì˜ê°¯ìˆ˜'] > 0).sum()
        print(f"[Day {day_num}] âœ“ Complete: {data_grids:,}/{len(all_grid_ids):,} grids with data")
        print(f"[Day {day_num}] âœ“ Saved to {output_file}")
        
        return output_file
        
    except Exception as e:
        print(f"[Day {day_num}] ERROR: {e}")
        import traceback
        traceback.print_exc()
        return None

def main():
    """Main pipeline."""
    start_time = datetime.now()
    
    print("=" * 70)
    print("PHASE 2: KD-Tree with Daily Save [OPTIMIZED] + SHP Export")
    print("=" * 70)
    print(f"Start time: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"CPU cores: {cpu_count()}, Using: {NUM_PROCESSES} processes")
    print(f"Daily SHP export: {'âœ“ Enabled' if SAVE_DAILY_SHP else 'âœ— Disabled'}")
    print(f"Final SHP export: {'âœ“ Enabled' if SAVE_FINAL_SHP else 'âœ— Disabled'}")
    
    if not os.path.exists(INPUT_DIR):
        print(f"âŒ ERROR: {INPUT_DIR} not found")
        return
    
    daily_files = sorted(glob.glob(os.path.join(INPUT_DIR, "filtered_*.csv")))
    if not daily_files:
        print(f"âŒ ERROR: No files in {INPUT_DIR}")
        return
    
    print(f"âœ“ Found {len(daily_files)} daily files")
    
    # Load grid
    print(f"\nLoading grid...")
    try:
        grid_df = pd.read_csv(GRID_PATH)
        grid_df['wkt'] = grid_df['wkt'].str.replace(';', ',')
        grid_df['geometry'] = grid_df['wkt'].apply(wkt.loads)
        gdf_grid = gpd.GeoDataFrame(grid_df, geometry='geometry', crs='EPSG:4326')
        
        # ì „ì²´ ê²©ì ID ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
        all_grid_ids = gdf_grid['MIN1'].tolist()
        
        print(f"âœ“ Loaded {len(gdf_grid):,} total grid cells")
        
        # Build KD-Tree
        print(f"Building KD-Tree...")
        kdtree, centroids = build_grid_kdtree(gdf_grid)
        print(f"âœ“ KD-Tree ready")
        
        # â­ í•µì‹¬ ìµœì í™”: grid lookup ë”•ì…”ë„ˆë¦¬ ìƒì„± (O(1) ì ‘ê·¼)
        print(f"Building grid lookup index (for O(1) access)...")
        grid_lookup = {
            i: {
                'geometry': gdf_grid.iloc[i].geometry,
                'grid_id': gdf_grid.iloc[i]['MIN1']
            }
            for i in range(len(gdf_grid))
        }
        print(f"âœ“ Grid lookup ready ({len(grid_lookup):,} grids indexed)")
        
        # â­ ë°”ìš´ë”© ë°•ìŠ¤ ë¯¸ë¦¬ ê³„ì‚°
        print(f"Precomputing grid bounds...")
        grid_bounds = {i: geom.bounds for i, geom in enumerate(gdf_grid.geometry)}
        print(f"âœ“ Grid bounds ready")
        
    except Exception as e:
        print(f"âŒ ERROR loading grid: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # Process in parallel
    print(f"\n{'=' * 70}")
    print(f"Processing {len(daily_files)} days in parallel...")
    print(f"Each day saved with ALL {len(all_grid_ids):,} grids")
    print(f"{'=' * 70}\n")
    
    # â­ grid_lookupê³¼ grid_boundsë¥¼ ì¸ìë¡œ ì „ë‹¬
    args_list = [(f, gdf_grid, kdtree, all_grid_ids, grid_lookup, grid_bounds) for f in daily_files]
    
    with Pool(NUM_PROCESSES) as pool:
        result_files = pool.map(process_single_day, args_list)
    
    valid_files = [f for f in result_files if f is not None]
    
    if not valid_files:
        print("âŒ No valid results")
        return
    
    print(f"\n{'=' * 70}")
    print(f"Merging {len(valid_files)} daily files...")
    print(f"{'=' * 70}")
    
    # ë‚ ì§œë³„ íŒŒì¼ ë³‘í•©
    daily_summaries = []
    all_shp_files = []
    
    for result_file in valid_files:
        try:
            df = pd.read_csv(result_file)
            daily_summaries.append(df)
            data_count = (df['ì„ ì˜ê°¯ìˆ˜'] > 0).sum()
            print(f"  {os.path.basename(result_file)}: {data_count:,}/{len(df):,} grids with data")
            
            # â­ SHP íŒŒì¼ ìˆ˜ì§‘
            if SAVE_DAILY_SHP:
                day_num = os.path.basename(result_file).replace('grid_summary_', '').replace('.csv', '')
                shp_file = os.path.join(DAILY_SHP_DIR, f"trajectories_{day_num}.shp")
                if os.path.exists(shp_file):
                    all_shp_files.append(shp_file)
        except Exception as e:
            print(f"  Warning: {result_file}: {e}")
    
    if not daily_summaries:
        print("âŒ No summaries to merge")
        return
    
    combined_df = pd.concat(daily_summaries, ignore_index=True)
    
    # ì „ì²´ ê¸°ê°„ ì§‘ê³„
    final_summary = combined_df.groupby('ê²©ìëª…').agg(
        ì„ ë°•ì„ ê¸¸ì´sum=('ì„ ë°•ì„ ê¸¸ì´sum', 'sum'),
        ì„ ì˜ê°¯ìˆ˜=('ì„ ì˜ê°¯ìˆ˜', 'sum')
    ).reset_index()
    
    final_summary['ì„ ë°•ì„ ê¸¸ì´sum'] = final_summary['ì„ ë°•ì„ ê¸¸ì´sum'].round(2)
    final_summary['ì„ ì˜ê°¯ìˆ˜'] = final_summary['ì„ ì˜ê°¯ìˆ˜'].astype(int)
    final_summary = final_summary.sort_values('ê²©ìëª…').reset_index(drop=True)
    
    # Save final
    final_summary.to_csv(FINAL_CSV, index=False, encoding='utf-8-sig')
    print(f"\nâœ“ Final CSV: {FINAL_CSV}")
    
    with open(FINAL_JSON, 'w', encoding='utf-8') as f:
        json.dump(final_summary.to_dict(orient='records'), f, ensure_ascii=False, indent=2)
    print(f"âœ“ Final JSON: {FINAL_JSON}")
    
    # â­ ì „ì²´ SHP ë³‘í•© (ì„ íƒì‚¬í•­)
    if SAVE_FINAL_SHP and all_shp_files:
        print(f"\nMerging all daily SHP files...")
        try:
            all_gdf = []
            for shp_file in sorted(all_shp_files):
                try:
                    gdf = gpd.read_file(shp_file)
                    all_gdf.append(gdf)
                    print(f"  Loaded: {os.path.basename(shp_file)} ({len(gdf):,} features)")
                except Exception as e:
                    print(f"  âš ï¸ Failed to load {shp_file}: {e}")
            
            if all_gdf:
                # ì „ì²´ ë³‘í•©
                final_gdf = pd.concat(all_gdf, ignore_index=True)
                
                # ìµœì¢… SHP ì €ì¥
                final_shp_file = os.path.join(FINAL_SHP_DIR, "all_trajectories.shp")
                final_gdf.to_file(final_shp_file, driver='SHAPEFILE', encoding='utf-8')
                print(f"\nâœ“ Final merged SHP: {final_shp_file} ({len(final_gdf):,} features)")
        except Exception as e:
            print(f"âš ï¸ Failed to merge SHP files: {e}")
    
    print(f"\nSample (first 10 with data):")
    sample = final_summary[final_summary['ì„ ì˜ê°¯ìˆ˜'] > 0].head(10)
    print(sample.to_string(index=False))
    
    end_time = datetime.now()
    print("\n" + "=" * 70)
    print("COMPLETE!")
    print("=" * 70)
    print(f"Duration: {end_time - start_time}")
    print(f"Total grids: {len(final_summary):,}")
    print(f"Grids with data: {(final_summary['ì„ ì˜ê°¯ìˆ˜'] > 0).sum():,}")
    print(f"Total length: {final_summary['ì„ ë°•ì„ ê¸¸ì´sum'].sum():,.2f} m")
    print(f"Total segments: {final_summary['ì„ ì˜ê°¯ìˆ˜'].sum():,}")
    
    print(f"\n{'=' * 70}")
    print("ğŸ“ Output Folders:")
    print(f"{'=' * 70}")
    print(f"\n1ï¸âƒ£  Daily CSV Results:")
    print(f"   {DAILY_RESULTS_DIR}")
    
    if SAVE_DAILY_SHP:
        print(f"\n2ï¸âƒ£  Daily SHP Trajectories (ë‚ ì§œë³„ ê¶¤ì ):")
        print(f"   {DAILY_SHP_DIR}")
        print(f"   Files: {len(all_shp_files)} SHP files")
        print(f"   â†’ QGis/ArcGisì—ì„œ ì§ì ‘ ì‹œê°í™” ê°€ëŠ¥!")
    
    print(f"\n3ï¸âƒ£  Summary Results:")
    print(f"   CSV: {FINAL_CSV}")
    print(f"   JSON: {FINAL_JSON}")
    
    if SAVE_FINAL_SHP:
        print(f"\n4ï¸âƒ£  Final Merged SHP (ì „ì²´ ê¶¤ì ):")
        print(f"   {FINAL_SHP_DIR}")
        print(f"   File: all_trajectories.shp")
        print(f"   â†’ ì „ì²´ ì„ ë°• ê¶¤ì  í•œ ë²ˆì— ì‹œê°í™”!")
    
    print(f"\n{'=' * 70}")
    print("ğŸ” View in GIS:")
    print(f"{'=' * 70}")
    print(f"  QGISì—ì„œ ë‹¤ìŒ íŒŒì¼ë“¤ì„ ì—´ ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
    if SAVE_DAILY_SHP:
        print(f"  - {DAILY_SHP_DIR}/*.shp (ë‚ ì§œë³„)")
    if SAVE_FINAL_SHP:
        print(f"  - {FINAL_SHP_DIR}/all_trajectories.shp (ì „ì²´)")

if __name__ == "__main__":
    main()